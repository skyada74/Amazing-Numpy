{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_ Numpy Implementation of Logistic Regression\n",
    "\n",
    "This notebook is to explain the implementation of logistic regression using only **Numpy** module. I will divide this problem into multiple sub-steps and explain each thing separately. \n",
    "\n",
    "Data used here is IRIS dataset available in sklearn module. Labels are converted into a binary variable where only label: 0 (Setosa) has been used as target for training and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Feature Space : (150, 4)\n",
      "Shape of Target Space: (150, 1)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = np.array([1 if i == 0 else 0 for i in iris.target]).reshape(-1, 1)\n",
    "print(\"Shape of Feature Space : {}\".format(X.shape))\n",
    "print(\"Shape of Target Space: {}\".format(y.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above, label column has been explicitly reshaped into a column vector. In **Numpy**, usually scalar values are represented in (m, ) shape which can sometime generate weird results when passed to a function. It's a good practice to have explicitly defined shapes for all vectors wherever possible.\n",
    "\n",
    "Also, since this is a small dataset, train and test set are simply created by choosing first 120 and last 30 rows. However, for larger datasets it is recommended to use a random selection method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_X, test_X = X[:120], X[120:]\n",
    "train_y, test_y = y[:120], y[120:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1_ General Knowledge\n",
    "\n",
    "<img src=\"LR.png\" style=\"width:650px;height:400px;\">\n",
    "\n",
    "Logistic regression is a model which uses a logistic function to classify a binary dependent variable. \n",
    "\n",
    "**Mathematical expression of the algorithm**:\n",
    "\n",
    "For one example $x^{(i)}$:\n",
    "$$z^{(i)} = w^T x^{(i)} + b \\tag{1}$$\n",
    "$$\\hat{y}^{(i)} = a^{(i)} = sigmoid(z^{(i)})\\tag{2}$$ \n",
    "$$ \\mathcal{L}(a^{(i)}, y^{(i)}) =  - y^{(i)}  \\log(a^{(i)}) - (1-y^{(i)} )  \\log(1-a^{(i)})\\tag{3}$$\n",
    "\n",
    "The cost is then computed by summing over all training examples:\n",
    "$$ J = \\frac{1}{m} \\sum_{i=1}^m \\mathcal{L}(a^{(i)}, y^{(i)})\\tag{4}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2_ Building the parts of our algorithm\n",
    "\n",
    "The main steps are:\n",
    "\n",
    "1. Define the model structure (such as number of input features) \n",
    "2. Initialize the model's parameters (weight matrix and bias)\n",
    "3. Loop:\n",
    "    - Calculate current loss (forward propagation)\n",
    "    - Calculate current gradient (backward propagation)\n",
    "    - Update parameters (gradient descent)\n",
    "    \n",
    "## 2.1_ Helper Functions\n",
    "\n",
    "**Function - Sigmoid**\n",
    "\n",
    "Purpose: Computes sigmoid of input z\n",
    "\n",
    "Arguments :\n",
    "\n",
    "        z -- a scalar or numpy array of any size\n",
    "        \n",
    "Return :\n",
    "\n",
    "        s -- sigmoid value of input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sigmoid of Zero: 0.5\n",
      "Sigmoid of Ten: 0.9999546021312976\n"
     ]
    }
   ],
   "source": [
    "def sigmoid(z):\n",
    "        \n",
    "    s = 1/(1 + np.exp(-z))\n",
    "    return s\n",
    "\n",
    "print(\"Sigmoid of Zero: {}\".format(sigmoid(0)))\n",
    "print(\"Sigmoid of Ten: {}\".format(sigmoid(10)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2_ Initializing Parameters\n",
    "\n",
    "**Function - initialize**\n",
    "\n",
    "Purpose : creates a vector of random numbers of shape (dim, 1) for weights and initializes bias with 0.\n",
    "\n",
    "Arguments :\n",
    "\n",
    "        dim -- size of the w vector we want (or number of parameters in this case)\n",
    "\n",
    "Returns :\n",
    "\n",
    "        w -- initialized vector of shape (dim, 1)\n",
    "  \n",
    "        b -- initialized scalar (corresponds to the bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize(dim):\n",
    "    \n",
    "    w = np.random.randn(dim, 1)\n",
    "    b = 0\n",
    "    return w, b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "w = [[ 0.29868666]\n",
      " [-1.3379613 ]\n",
      " [ 0.64811609]\n",
      " [ 1.15116898]]\n",
      "b = 0\n"
     ]
    }
   ],
   "source": [
    "w, b = initialize(4)\n",
    "print(\"w = \" + str(w))\n",
    "print(\"b = \" + str(b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3_ Calculate Gradients and Loss\n",
    "\n",
    "Now that the parameters are initialized, a forward pass of the data must be done in order to calculate error and gradient of the error which will help in learning the parameters.\n",
    "\n",
    "Forward Propagation:\n",
    "- Take X\n",
    "- Compute $A = \\sigma(X * w + b) = (a^{(1)}, a^{(2)}, ..., a^{(m-1)}, a^{(m)})$\n",
    "- Calculate the cost function: $J = -\\frac{1}{m}\\sum_{i=1}^{m}y^{(i)}\\log(a^{(i)})+(1-y^{(i)})\\log(1-a^{(i)})$\n",
    "\n",
    "Two formulas which will be used here for calculation of gradient are:\n",
    "\n",
    "$$ \\frac{\\partial J}{\\partial w} = \\frac{1}{m}X^T(A-Y)\\tag{5}$$\n",
    "$$ \\frac{\\partial J}{\\partial b} = \\frac{1}{m} \\sum_{i=1}^m (a^{(i)}-y^{(i)})\\tag{6}$$\n",
    "\n",
    "**Function - propagate**\n",
    "\n",
    "Purpose : Calcualte cost function and its gradient for each pass of data\n",
    "\n",
    "Arguments :\n",
    "\n",
    "        w -- weights, a numpy array\n",
    "        b -- bias, a scalar\n",
    "        X -- features\n",
    "        Y -- label\n",
    "\n",
    "Return : \n",
    "\n",
    "        cost -- negative log-likelihood cost for logistic regression\n",
    "        dw -- gradient of the loss with respect to w, thus same shape as w\n",
    "        db -- gradient of the loss with respect to b, thus same shape as b "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: propagate\n",
    "\n",
    "def propagate(w, b, X, Y):\n",
    " \n",
    "    m = X.shape[0]\n",
    "    A = sigmoid(np.dot(X, w) + b)  # compute logistic value\n",
    "    cost = (- 1 / m) * np.sum(Y * np.log(A) + (1 - Y) * (np.log(1 - A)))  # compute negative logloss \n",
    "    \n",
    "    ### Calculate gradient ###\n",
    "    dw = (1 / m) * np.dot(X.T, (A - Y))\n",
    "    db = (1 / m) * np.sum(A - Y)\n",
    "    \n",
    "    ## Return values ##\n",
    "    grads = {\"dw\": dw, \"db\": db}\n",
    "    \n",
    "    return grads, cost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'dw': array([[1.52741652],\n",
       "         [0.29162463],\n",
       "         [2.01788092],\n",
       "         [0.7494527 ]]), 'db': 0.18372204544164408}, 2.5441962009194365)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "propagate(w, b, train_X, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.4_ Optimization\n",
    "\n",
    "Now the next step is to update the parameters. The goal is to learn $w$ and $b$ by minimizing the cost function $J$. For a parameter $\\theta$, the update rule is $ \\theta = \\theta - \\alpha \\text{ } d\\theta$, where $\\alpha$ is the learning rate\n",
    "\n",
    "**Function - optimize**\n",
    "\n",
    "Purpose : optimizes w and b by running a gradient descent algorithm\n",
    "\n",
    "Arguments : \n",
    "        \n",
    "        w -- weights, a numpy array\n",
    "        b -- bias, a scalar\n",
    "        X -- features\n",
    "        Y -- label\n",
    "        num_iterations -- number of times data should be passed to minimize the loss\n",
    "        learning_rate -- learning rate of the gradient descent update rule\n",
    "        print_cost -- True to print the loss every 100 steps\n",
    "\n",
    "Returns : \n",
    "        \n",
    "        params -- dictionary containing the weights w and bias b\n",
    "        grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n",
    "        costs -- list of all the costs computed during the optimization, this will be used to plot the learning curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimize(w, b, X, Y, num_iterations, learning_rate, print_cost = False):\n",
    "    \n",
    "    costs = []\n",
    "    \n",
    "    for i in range(num_iterations):\n",
    "        \n",
    "        # Cost and gradient calculated in each iteration\n",
    "        grads, cost = propagate(w, b, X, Y)\n",
    "        \n",
    "        # Retrieve derivatives from grads\n",
    "        dw = grads[\"dw\"]\n",
    "        db = grads[\"db\"]\n",
    "        \n",
    "        # update rule\n",
    "        w = w - learning_rate * dw  \n",
    "        b = b - learning_rate * db\n",
    "        \n",
    "        # Record the costs\n",
    "        if i % 100 == 0:\n",
    "            costs.append(cost)\n",
    "        \n",
    "        # Print the cost every 100 training examples\n",
    "        if print_cost and i % 100 == 0:\n",
    "            print (\"Cost after iteration {} : {}\".format(i, cost))\n",
    "    \n",
    "    params = {\"w\": w, \"b\": b}\n",
    "    \n",
    "    return params, costs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'w': array([[ 0.9655611 ],\n",
       "         [ 1.11827664],\n",
       "         [-3.15882581],\n",
       "         [-0.54523699]]), 'b': 0.442167435844594}, [2.5441962009194365])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "optimize(w, b, train_X, train_y, 100, 0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.5_ Predict\n",
    "\n",
    "**Function - predict**\n",
    "\n",
    "Purpose : Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n",
    "\n",
    "Arguments : \n",
    "        \n",
    "        w -- weights, a numpy array\n",
    "        b -- bias, a scalar\n",
    "        X -- features\n",
    "\n",
    "Returns : \n",
    "\n",
    "        pred -- a numpy array (vector) containing all predictions (0/1) based on a threshold\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(w, b, X, threshold = 0.5):\n",
    "        \n",
    "    m = X.shape[0]\n",
    "    pred = np.zeros((m, 1))\n",
    "    \n",
    "    # Compute vector \"A\" predicting the probabilities\n",
    "    A = sigmoid(np.dot(X, w) + b)\n",
    "    \n",
    "    for i in range(A.shape[0]):\n",
    "        # Convert probabilities to actual predictions\n",
    "        pred[i, 0] = 1 if A[i, 0] > threshold else 0\n",
    "    \n",
    "    return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3_ Merge all into a model\n",
    "\n",
    "Now is the time to put all building blocks together in the right order.\n",
    "\n",
    "**model** \n",
    "\n",
    "Purpose : Builds the logistic regression model by calling functions defined before\n",
    "\n",
    "Arguments :\n",
    "       \n",
    "        X_train -- training set represented by a numpy array \n",
    "        Y_train -- training labels represented by a numpy array (vector) \n",
    "        X_test -- test set represented by a numpy array \n",
    "        Y_test -- test labels represented by a numpy array (vector) \n",
    "        num_iterations -- hyperparameter representing the number of iterations to optimize the parameters\n",
    "        learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n",
    "        print_cost -- Set to true to print the cost every 100 iterations\n",
    "        \n",
    "Returns :\n",
    "        \n",
    "        d -- dictionary containing information about the model\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(X_train, Y_train, X_test, Y_test, num_iterations=2000, learning_rate=0.5, print_cost=False):\n",
    "\n",
    "    # initialize parameters \n",
    "    w, b = initialize(X_train.shape[1])\n",
    "\n",
    "    # Gradient descent\n",
    "    parameters, costs = optimize(w, b, X_train, Y_train, num_iterations, learning_rate, print_cost)\n",
    "    \n",
    "    # Retrieve parameters w and b from dictionary \"parameters\"\n",
    "    w = parameters[\"w\"]\n",
    "    b = parameters[\"b\"]\n",
    "    \n",
    "    # Predict test/train set examples \n",
    "    pred_test = predict(w, b, X_test)\n",
    "    pred_train = predict(w, b, X_train)\n",
    "\n",
    "    # Print train/test Errors\n",
    "    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(pred_train - Y_train)) * 100))\n",
    "    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(pred_test - Y_test)) * 100))\n",
    "  \n",
    "    d = {\"costs\": costs,\n",
    "         \"Y_prediction_test\": pred_test, \n",
    "         \"Y_prediction_train\" : pred_train, \n",
    "         \"w\" : w, \n",
    "         \"b\" : b,\n",
    "         \"learning_rate\" : learning_rate,\n",
    "         \"num_iterations\": num_iterations}\n",
    "    \n",
    "    return d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost after iteration 0 : 5.169811708647467\n",
      "Cost after iteration 100 : 0.012362593931101125\n",
      "Cost after iteration 200 : 0.0069643139305350064\n",
      "Cost after iteration 300 : 0.004914024031873643\n",
      "Cost after iteration 400 : 0.0038230708049046933\n",
      "Cost after iteration 500 : 0.0031419440833102253\n",
      "Cost after iteration 600 : 0.002674535815169117\n",
      "Cost after iteration 700 : 0.0023330344901463062\n",
      "Cost after iteration 800 : 0.0020721163935097536\n",
      "Cost after iteration 900 : 0.0018659714153750077\n",
      "Cost after iteration 1000 : 0.001698798656718109\n",
      "Cost after iteration 1100 : 0.0015603714344647399\n",
      "Cost after iteration 1200 : 0.0014437719361550135\n",
      "Cost after iteration 1300 : 0.0013441493591226908\n",
      "Cost after iteration 1400 : 0.0012579992626569171\n",
      "Cost after iteration 1500 : 0.0011827253146116166\n",
      "Cost after iteration 1600 : 0.0011163620886007679\n",
      "Cost after iteration 1700 : 0.0010573937616868863\n",
      "Cost after iteration 1800 : 0.0010046320641398823\n",
      "Cost after iteration 1900 : 0.0009571320252150256\n",
      "train accuracy: 100.0 %\n",
      "test accuracy: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "d = model(train_X, train_y, test_X, test_y, print_cost = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4_ Conclusion\n",
    "\n",
    "This work is not a complete method for Logistic Regression. It has been developed only for understanding the most common and essential operations involved in an LR model. Few things which are still lacking in this implementation are:\n",
    "\n",
    "1. Calculation of p-values or any variable static to understand the importance of it in the model\n",
    "2. Calculation of testing loss in order to check for over/under fitting\n",
    "3. Integration of any regularization method to help in building a robust model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
